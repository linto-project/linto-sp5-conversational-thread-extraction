{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: \n",
    "\n",
    "I) version base\n",
    "(partiel)- sauver prédictions sur dev (ou test); script\n",
    "\n",
    "      \n",
    "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/data/dataset_readers/semantic_dependency_parsing.py\n",
    "\n",
    "\n",
    "améliorations: \n",
    "- encodeur tour -> bert\n",
    "     deux solutions \n",
    "     1) bertpooler direct sur passthrough embeddings \n",
    "     2) indexer/tokenizer bert + seq2vec ?  (pb de stockage tokens...)\n",
    "- preprocessing des chats\n",
    "    - turn level feature extraction + intégration au modèle\n",
    "        - (done) 1 trait (is_server)\n",
    "        - generaliser\n",
    "    - pair level feat. extr. + integration au modèle\n",
    "        cf coref model\n",
    "         - (done) avec 1 ou 2 traits (distance + test si target addresse = src speaker)\n",
    "         - https://github.com/allenai/allennlp-models/blob/master/allennlp_models/coref/models/coref.py\n",
    "         - https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py (for distance etc)\n",
    "    - (done) alternative pour distance: embedding de la position avec sinusoides\n",
    "    - ajouter embedding/encodage niveau caractères vue la nature du chat\n",
    "    - utiliser les embeddings glove entrainés sur ubuntu fournis par kummerfeld \n",
    "- changer les instances\n",
    "    - small sequence (10-15 tours), predire un seul lien \n",
    "         - (a) pour chaque source: contexte après, prédire lien chaque tour / src\n",
    "         - (b) pour chaque target: contexte avant, prédire lien, chaque tour/target / garder le meilleur\n",
    "- test softmax vs sigmoid in scores->probs\n",
    "- better decoders\n",
    "    - mst \n",
    "    - (done) best head for each target -> pas bcp mieux\n",
    "- clean-up\n",
    "     - metadata in instances: tokens, etc\n",
    "     - model saving/loading from archive\n",
    "     - separate prediction in dedicated script\n",
    "\n",
    "data analysis:\n",
    "\n",
    "  - eval in kummerfeld: is it for each \"query\" ? (query=turn with question ?)\n",
    "      -> actually more complicated cf the supplementary material to the acl paper\n",
    "  \n",
    "  - chats are not annotated before the 1000th turn, then \n",
    "      - 500 turns for train and test\n",
    "      - 250 for dev (-> hence better scores on dev)\n",
    "      - what the fuck ? \n",
    "  - important features for turn-turn linking\n",
    "      - same speaker (pair)\n",
    "      - emoticon-only turn (often just a follow-up of previous turn) (turn)\n",
    "      - speaker id mention (usually starting a turn \"bob: blabla\") is the same (pair)\n",
    "      - speaker id mention in the turn-> generally indicate longer attachment (turn)\n",
    "      - speaker id mention is same as speaker of other turn (pair)\n",
    "      - time distance (pair)\n",
    "      - nb of turn apart (pair)\n",
    "      - presence of link -> should be preprocessed as a unique token (turn)\n",
    "      - analysis de corrélation avec attachement pour sélection des traits\n",
    "      - users ubotu/ubottu -> bots après commande/d'aide eg \"!alsaconf\"\n",
    "      - tour commence par \"!\": generalement pour aider tour précédent, et automatiquement suivi de la réponse du bot ubuntu\n",
    "  - to be checked: \n",
    "      - inconsistence of SELFLOOP / NO LINK ? cf guidelines\n",
    "          - \"If it appears to respond to something, but you can't find it, leave no link.\"\n",
    "          - Interjections / Noise -> idem\n",
    "      - nb of multiple heads. seems minor actually -> important to find best head\n",
    "      - distribution of probs for best head candidate -> in relation to presence of link, to check correlation\n",
    "      - check vocab / and embedding of OOV / what is done with them; would be sort of solved with BERT piece tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Tuple, Dict, List, Iterable\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import Field, LabelField, TextField, ListField, SequenceLabelField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, PretrainedBertIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "\n",
    "from allennlp.modules import Seq2VecEncoder, Seq2SeqEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.common import Params\n",
    "from allennlp.nn import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dummy_chat_reader import ChatReader\n",
    "from irc_chat_reader import ChatReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "tokenizer_cfg = Params({\"word_splitter\": {\"language\": \"en\"}})\n",
    "\n",
    "word_tokenizer = Tokenizer.from_params(tokenizer_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT:\n",
    "    bert_model = \"bert-base-uncased\"\n",
    "    bert_cfg = {\n",
    "        \"tokenizer\": {\n",
    "                \"type\": \"pretrained_transformer\",\n",
    "                \"model_name\": bert_model,\n",
    "                \"do_lowercase\": True, \n",
    "                },\n",
    "        \"token_indexers\": {\n",
    "                        \"type\": \"bert-pretrained\",\n",
    "                        \"pretrained_model\": bert_model,\n",
    "                        \"do_lowercase\": True, \n",
    "                        \"truncate_long_sequences\":True,\n",
    "                        \"use_starting_offsets\": True\n",
    "\n",
    "            },\n",
    "        \"text_field_embedder\": {\n",
    "                \"allow_unmatched_keys\": True,\n",
    "                \"embedder_to_indexer_map\": {\n",
    "                \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            },\n",
    "        \"token_embedders\": {\n",
    "                \"bert\": {\n",
    "                    \"type\": \"bert-pretrained\",\n",
    "                    \"pretrained_model\": bert_model \n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    token_indexers = TokenIndexer.from_params(Params(bert_cfg[\"token_indexers\"]))\n",
    "    word_tokenizer = Tokenizer.from_params(Params(bert_cfg[\"tokenizer\"]))\n",
    "    #vocab ??? see below\n",
    "    #word_embedding = TextFieldEmbedder.from_params(vocab,Params(bert_cfg[\"text_field_embedder\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT:\n",
    "    from types import SimpleNamespace\n",
    "    bert_indexer = PretrainedBertIndexer(\n",
    "            pretrained_model=\"bert-base-uncased\",\n",
    "            do_lowercase=True,\n",
    "            truncate_long_sequences=True\n",
    "        )\n",
    "    token_indexers = {\"bert\":bert_indexer}\n",
    "    # crucial piece since bert indexer acts as a tokenizer too\n",
    "    def word_tokenizer_fun(s: str):\n",
    "            return [Token(t) for t in bert_indexer.wordpiece_tokenizer(s)]\n",
    "    # added to match irc_chat_reader expectations of a tokenizer\n",
    "    word_tokenizer = SimpleNamespace(tokenize=word_tokenizer_fun)\n",
    "\n",
    "\n",
    "    bert_embedder = PretrainedBertEmbedder(\n",
    "            pretrained_model=\"bert-base-uncased\",\n",
    "            top_layer_only=False,\n",
    "        )\n",
    "    word_embedder: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                {\"embedder_to_indexer_map\": {\n",
    "                                                                 \"bert\": [\"bert\", \"bert-offsets\"]},\n",
    "                                                                \"tokens\": {\"input_ids\": \"tokens\",\n",
    "                                                                              \"offsets\": \"offset\"}},\n",
    "                                                                allow_unmatched_keys=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [00:28,  5.28it/s]\n",
      "10it [00:00, 11.72it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = ChatReader(\n",
    "    tokenizer=word_tokenizer,\n",
    "    token_indexers=token_indexers,\n",
    "    raw = True,\n",
    "    sub_sequence = None,\n",
    "    #clip = 200\n",
    "    )\n",
    "train_instances = reader.read(\"../data/train\")\n",
    "dev_instances = reader.read(\"../data/dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:00<00:00, 313.93it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_instances+dev_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in train_instances:\n",
    "#    #print(i)\n",
    "#    i[\"lines\"].index(vocab)\n",
    "#    i[\"arcs\"].index(vocab)\n",
    "#    print(i[\"lines\"].get_padding_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i0 = train_instances[0]\n",
    "i0[\"is_server\"].array[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while True:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum([len([x for x in i[\"metadata\"].metadata[\"addressees\"] if x is None]) for i in train_instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of document with zeros edges: 0\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for n,i in enumerate(train_instances):\n",
    "    nbarcs = len(i[\"arcs\"].indices)\n",
    "    if nbarcs==0: \n",
    "        c+=1 \n",
    "        print(n)\n",
    "print(\"nb of document with zeros edges:\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance max\n",
    "#for instance in train_instances:\n",
    "#    maxl = max([abs(j-i) for (i,j) in instance[\"arcs\"].indices])\n",
    "#    print(maxl)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/miniconda3/envs/allennlp2/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "30635it [00:00, 111245.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61261it [00:00, 222682.06it/s]\n"
     ]
    }
   ],
   "source": [
    "input_size = 50\n",
    "\n",
    "if not BERT:\n",
    "    turn_encoder_cfg = Params({\"type\":\"gru\",'input_size': input_size, 'hidden_size': 100, 'num_layers': 1,\n",
    "                      'dropout': 0.25, 'bidirectional': True\n",
    "})\n",
    "    #can be changed dynamically encoder_cfg[\"type\"] = \"lstm\"\n",
    "    # warning: if bidirectional, state output dimension is hidden_size x 2 \n",
    "\n",
    "    turn_encoder = Seq2VecEncoder.from_params(turn_encoder_cfg)\n",
    "    turn_encoder.hidden_size = turn_encoder_cfg[\"hidden_size\"]*(1+turn_encoder_cfg[\"bidirectional\"])\n",
    "else:\n",
    "    turn_encoder = BertPooler(\"bert-base-uncased\",requires_grad=True,dropout=0.0)\n",
    "    turn_encoder.hidden_size = turn_encoder._embedding_dim\n",
    "\n",
    "print(turn_encoder.hidden_size)\n",
    "\n",
    "\n",
    "turn_feature_size = 1\n",
    "\n",
    "chat_encoder_cfg = Params({\"type\":\"gru\",'input_size': turn_encoder.hidden_size, 'hidden_size': 100, 'num_layers': 3,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "chat_encoder = Seq2SeqEncoder.from_params(chat_encoder_cfg)\n",
    "chat_encoder.hidden_size = chat_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "if not BERT:\n",
    "    #glove_dim, glove_version = 300, \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.300d.txt.gz\"\n",
    "    glove_dim, glove_version = input_size, \"../data/glove-ubuntu.txt\"\n",
    "    glove_text_field_embedder = Embedding.from_params(vocab,Params({\"pretrained_file\": glove_version,\n",
    "                                                          \"embedding_dim\": glove_dim,\n",
    "                                                          \"trainable\": True\n",
    "    }))\n",
    "    \n",
    "\n",
    "    #token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "    #                        embedding_dim=300)\n",
    "    token_embedding = glove_text_field_embedder \n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT: \n",
    "    word_embeddings = word_embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from biaffine parser, another config (not used: assumes tokens)\n",
    "chat_encoder_cfg =  Params({\n",
    "            \"type\": \"stacked_bidirectional_lstm\",\n",
    "            \"hidden_size\": 400,\n",
    "            \"input_size\": 200,\n",
    "            \"num_layers\": 3,\n",
    "            \"recurrent_dropout_probability\": 0.3,\n",
    "            \"use_highway\": True\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from typing import Dict, List, Iterable\n",
    "from allennlp.modules import TimeDistributed\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.common.checks import check_dimensions_match, ConfigurationError\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder, Embedding, InputVariationalDropout\n",
    "from allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention\n",
    "from allennlp.modules.matrix_attention.dot_product_matrix_attention import DotProductMatrixAttention\n",
    "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
    "\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import InitializerApplicator, Activation\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.nn.util import add_positional_features\n",
    "\n",
    "\n",
    "#???? TODO from allennlp.nn.util import min_value_of_dtype -> only allennlp >= 1.0\n",
    "def min_value_of_dtype(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.\n",
    "    \"\"\"\n",
    "    return info_value_of_dtype(dtype).min\n",
    "def info_value_of_dtype(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    Returns the `finfo` or `iinfo` object of a given PyTorch data type. Does not allow torch.bool.\n",
    "    \"\"\"\n",
    "    if dtype == torch.bool:\n",
    "        raise TypeError(\"Does not support torch.bool\")\n",
    "    elif dtype.is_floating_point:\n",
    "        return torch.finfo(dtype)\n",
    "    else:\n",
    "        return torch.iinfo(dtype)\n",
    "\n",
    "# utility for max decoding. should be a better way but...\n",
    "# should also not modify the original tensor ?\n",
    "def force_max(t):\n",
    "    \"\"\"set all values of a tensor (batch,length,length) to zero if not maximum (on a given line)\"\"\"\n",
    "    dims = t.size()\n",
    "    b,_ = t.max(axis=2)\n",
    "    bb = b.repeat((1,dims[1]))\n",
    "    bb = torch.reshape(bb,(dims[0],dims[1],dims[1])).transpose(1,2)\n",
    "    #print(bb)\n",
    "    t[t<bb]=0\n",
    "    return t\n",
    "    \n",
    "    \n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.nn.util import get_lengths_from_binary_sequence_mask\n",
    "from allennlp.training.metrics import F1Measure\n",
    "\n",
    "import copy\n",
    "from overrides import overrides\n",
    "import torch\n",
    "from torch.nn.modules import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "import numpy\n",
    "import pandas as pds\n",
    "\n",
    "class ChatGraphParser(Model):\n",
    "    \"\"\"\n",
    "    A Parser for arbitrary graph structures.\n",
    "\n",
    "    Registered as a `Model` with name \"graph_parser\".\n",
    "\n",
    "    # Parameters\n",
    "\n",
    "    vocab : `Vocabulary`, required\n",
    "        A Vocabulary, required in order to compute sizes for input/output projections.\n",
    "    text_field_embedder : `TextFieldEmbedder`, required\n",
    "        Used to embed the `tokens` `TextField` we get as input to the model.\n",
    "    turn_encoder : `Seq2VeqEncoder`\n",
    "        The encoder that we will use to generate representation for whole turns from tokens.\n",
    "    chat_encoder: `Seq2SeqEncoder`   The encoder that we will use to generate representations\n",
    "        of turns within a chat\n",
    "    arc_representation_dim : `int`, required.\n",
    "        The dimension of the MLPs used for arc prediction.\n",
    "        \n",
    "    feature_size : `int`\n",
    "        The embedding size for all the embedded features, such as distances\n",
    "        \n",
    "    tag_feedforward : `FeedForward`, optional, (default = None).\n",
    "        The feedforward network used to produce tag representations.\n",
    "        By default, a 1 layer feedforward network with an elu activation is used.\n",
    "    arc_feedforward : `FeedForward`, optional, (default = None).\n",
    "        The feedforward network used to produce arc representations.\n",
    "        By default, a 1 layer feedforward network with an elu activation is used.\n",
    "\n",
    "    dropout : `float`, optional, (default = 0.0)\n",
    "        The variational dropout applied to the output of the encoder and MLP layers.\n",
    "    input_dropout : `float`, optional, (default = 0.0)\n",
    "        The dropout applied to the embedded text input.\n",
    "    edge_prediction_threshold : `int`, optional (default = 0.5)\n",
    "        The probability at which to consider a scored edge to be 'present'\n",
    "        in the decoded graph. Must be between 0 and 1.\n",
    "    initializer : `InitializerApplicator`, optional (default=`InitializerApplicator()`)\n",
    "        Used to initialize the model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Vocabulary,\n",
    "        text_field_embedder: TextFieldEmbedder,\n",
    "        turn_encoder: Seq2VecEncoder, \n",
    "        chat_encoder: Seq2SeqEncoder,\n",
    "        arc_representation_dim: int,\n",
    "        feature_size = 4,\n",
    "        arc_feedforward: FeedForward = None,\n",
    "        use_features = False,\n",
    "        turn_feature_size = 0,# total dimension of turn-based additional features\n",
    "        pair_feature_size = 0,# total dimension of pair-based additional features\n",
    "        dropout: float = 0.3,\n",
    "        input_dropout: float = 0.0,\n",
    "        edge_prediction_threshold: float = 0.5,\n",
    "        positive_class_weight = 40,\n",
    "        prediction_window = 13, # dont predict edge further apart\n",
    "        initializer: InitializerApplicator = InitializerApplicator(),\n",
    "        debug = False,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(vocab, **kwargs)\n",
    "        \n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.turn_encoder = TimeDistributed(turn_encoder)\n",
    "        self.chat_encoder = chat_encoder\n",
    "        \n",
    "        self.edge_prediction_threshold = edge_prediction_threshold\n",
    "        if not 0 < edge_prediction_threshold < 1:\n",
    "            raise ConfigurationError(\n",
    "                f\"edge_prediction_threshold must be between \"\n",
    "                f\"0 and 1 (exclusive) but found {edge_prediction_threshold}.\"\n",
    "            )\n",
    "\n",
    "        encoder_dim = chat_encoder.get_output_dim()\n",
    "        \n",
    "        self.use_features = use_features\n",
    "        self.turn_feature_size = turn_feature_size\n",
    "        self.pair_feature_size = pair_feature_size\n",
    "        \n",
    "        self.head_arc_feedforward = FeedForward(\n",
    "            encoder_dim+self.turn_feature_size, 1, arc_representation_dim, Activation.by_name(\"relu\")()\n",
    "        )\n",
    "        self.child_arc_feedforward = copy.deepcopy(self.head_arc_feedforward)\n",
    "\n",
    "        # 10 possible distance buckets.\n",
    "        if self.use_features:\n",
    "            self._num_distance_buckets = 10\n",
    "            self._distance_embedding = Embedding(\n",
    "                embedding_dim=feature_size, num_embeddings=self._num_distance_buckets\n",
    "            )\n",
    "            total_feature_size = self.pair_feature_size\n",
    "        \n",
    "        #\n",
    "        #self.arc_attention = BilinearMatrixAttention(\n",
    "        #    arc_representation_dim, arc_representation_dim, use_input_biases=True\n",
    "        #)\n",
    "        #self.arc_attention = DotProductMatrixAttention()\n",
    "        self.arc_attention = LinearMatrixAttention(\n",
    "            arc_representation_dim, arc_representation_dim, combination=\"x,y,x*y,x-y\", activation = None\n",
    "        )\n",
    "           \n",
    "        \n",
    "        self._dropout = InputVariationalDropout(dropout)\n",
    "        self._input_dropout = Dropout(input_dropout)\n",
    "        # unused for now\n",
    "        self.final_activation = Activation.by_name(\"sigmoid\")\n",
    "        # tested with distance feature (embedding) and spk_add boolean; final feedforward accounts also for attention scores\n",
    "        if self.use_features:\n",
    "            self.final_score = TimeDistributed(Linear(1+total_feature_size,1))#,self.final_activation))\n",
    "        \n",
    "        representation_dim = turn_encoder.get_output_dim()\n",
    "\n",
    "        self._unlabelled_f1 = F1Measure(positive_label=1)\n",
    "        #  with weight favoring recall of positive class \n",
    "        self._arc_loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\",\n",
    "                                                    pos_weight=torch.tensor([positive_class_weight]))\n",
    "        \n",
    "        self.prediction_window = prediction_window\n",
    "        initializer(self)\n",
    "        # useful for debugging\n",
    "        self.iter_count = 0 \n",
    "        self.debug = debug\n",
    "    # init done\n",
    "        \n",
    "    # todo \n",
    "    @overrides\n",
    "    def forward(\n",
    "        self,  # type: ignore\n",
    "        lines,\n",
    "        arcs: torch.LongTensor = None,\n",
    "        rel_features = None,\n",
    "        offsets = None,\n",
    "        is_server = None,\n",
    "        metadata: List[Dict[str, Any]] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameters\n",
    "\n",
    "        lines: the chat as a list of turns, each being a list of token\n",
    "        TODO: add metadata to instances\n",
    "        metadata : List[Dict[str, Any]], optional (default = None)\n",
    "            A dictionary of metadata for each batch element which has keys:\n",
    "                tokens : `List[str]`, required.\n",
    "                    The original string tokens in the sentence.\n",
    "        arcs : a tensor containing the adjacency matrix for the instance dependencies between turns\n",
    "            Has shape `(batch_size, sequence_length, sequence_length)`.\n",
    "\n",
    "        # Returns\n",
    "\n",
    "        An output dictionary.\n",
    "        \"\"\"\n",
    "        #########\n",
    "        # this is the part where chat is encoded as sequence of turn encodings\n",
    "        #########\n",
    "        # mask for each turn of each chat of the batch: shape = (batch_size x max_turns x tokens)\n",
    "        token_mask = get_text_field_mask(lines,num_wrapping_dims=1)\n",
    "\n",
    "        # chat turns fetching embedding\n",
    "        # turns_embedding tensor is (batch_size x turns x max tokens x token embedding size)\n",
    "        turns_embeddings = self.text_field_embedder(lines,num_wrapping_dims=1)\n",
    "      \n",
    "        # encoding turns\n",
    "        # turn_h has shape (batch_size x turns x encoder_output_size) \n",
    "        turn_h = self.turn_encoder(turns_embeddings,token_mask)\n",
    "        \n",
    "        # mask for chats is now nb of turns; beware weird return type of torch.max (tuple) \n",
    "        chat_mask = token_mask.max(axis=2)[0]\n",
    "        \n",
    "        # renaming to mask -> easier to transpose the rest of graph_parser\n",
    "        mask = chat_mask\n",
    "        \n",
    "        # graph parser goes on\n",
    "        # leave input dropout for now\n",
    "        # embedded_text_input = turn_h equivalent in hierarchical sequence -> renaming \n",
    "        #embedded_text_input = self._input_dropout(embedded_text_input)\n",
    "        embedded_text_input = turn_h\n",
    "        \n",
    "        # encoded_turns = encoded chat = self.chat_encoder(turn_h,chat_mask) equivalent in hierarchical sequence\n",
    "        encoded_turns = self.chat_encoder(embedded_text_input, mask)\n",
    "        #breakpoint()\n",
    "        encoded_turns = self._dropout(encoded_turns)\n",
    "        # experimentally, add positional sinusoidal encoding\n",
    "        encoded_turns = add_positional_features(encoded_turns)\n",
    "        \n",
    "        if self.use_features:\n",
    "            encoded_turns = torch.cat([encoded_turns,is_server.unsqueeze(-1)],dim=-1)\n",
    "    \n",
    "        # shape (batch_size, sequence_length, arc_representation_dim)\n",
    "        head_arc_representation = self._dropout(self.head_arc_feedforward(encoded_turns))\n",
    "        child_arc_representation = self._dropout(self.child_arc_feedforward(encoded_turns))\n",
    "\n",
    "        # features for distance between candidate turns (unused for now)\n",
    "        # should be refactored in a pair embedding dedicated function to integrate everything\n",
    "        if self.use_features:\n",
    "            distance_embeddings = self._distance_embedding(\n",
    "                util.bucket_values(offsets, num_total_buckets=self._num_distance_buckets)\n",
    "                )\n",
    "        # todo: change to infersent composition\n",
    "        #pair_embeddings = torch.cat(\n",
    "        #    [\n",
    "        #        target_embeddings,\n",
    "        #        src_embeddings,\n",
    "        #        src_embeddings * target_embeddings,\n",
    "        #        distance_embeddings,\n",
    "        #    ],\n",
    "        #    -1,\n",
    "        #)\n",
    "        \n",
    "        # Todo: change to simpler composition\n",
    "        # shape (batch_size, sequence_length, sequence_length)\n",
    "        arc_scores = self.arc_attention(head_arc_representation, child_arc_representation) \n",
    "        #breakpoint()\n",
    "        # result here is size (batch,seq length,seq length, 1+total_feature_dim)\n",
    "        if self.use_features: \n",
    "            final_rep = torch.cat([arc_scores.unsqueeze(-1),distance_embeddings,rel_features.unsqueeze(-1)],dim=-1)\n",
    "            final_size = final_rep.size()\n",
    "            # flattent seq lengthxlength to pass on to timedistributed\n",
    "            final_rep_flat = final_rep.flatten(1,2)\n",
    "            # should output batch x seqlenght*seqlength (x 1)\n",
    "            #if self.debug:\n",
    "            #    breakpoint()\n",
    "            final_scores = self.final_score(final_rep_flat)\n",
    "            # reshape to original matrix form batch x seqlength x seqlength\n",
    "            final_scores = torch.reshape(final_scores,(final_size[0],final_size[1],final_size[2],1)).squeeze(-1)\n",
    "            arc_scores = final_scores\n",
    "        \n",
    "        # shape (batch_size, num_tags, sequence_length, sequence_length)\n",
    "        #arc_tag_logits = self.tag_bilinear(head_tag_representation, child_tag_representation)\n",
    "        # Switch to (batch_size, sequence_length, sequence_length, num_tags)\n",
    "        #arc_tag_logits = arc_tag_logits.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        # Since we'll be doing some additions, using the min value will cause underflow\n",
    "        # CHAT: unncessary since we dont have a loss for labels\n",
    "        #minus_mask = ~mask * min_value_of_dtype(arc_scores.dtype) / 10\n",
    "        #arc_scores = arc_scores + minus_mask.unsqueeze(2) + minus_mask.unsqueeze(1)\n",
    "\n",
    "        self.iter_count += 1\n",
    "        # Debugging every x batchs \n",
    "        if self.debug and self.iter_count%(10)==0:\n",
    "            breakpoint()\n",
    "            \n",
    "        arc_probs = self._greedy_decode(arc_scores, mask,self.prediction_window)\n",
    "        # heads: \n",
    "        # values , heads = arc_probs.max(dim=2)\n",
    "        # should also include: threshold to have \n",
    "        # eg heads = heads[:,values>threshold] (beware of batch dimension!)\n",
    "        # -> link to loss ? none ? \n",
    "        # but check attachement score for metric (needs tag though could be reproduced w/o them)\n",
    "        \n",
    "        output_dict = {\"arc_probs\": arc_probs, \"mask\": mask}\n",
    "\n",
    "        if metadata:\n",
    "            output_dict[\"metadata\"] = metadata\n",
    "\n",
    "        arc_tags = arcs # gold labels -> here just the adjacency matrix 0/1 ? \n",
    "        if arc_tags is not None:\n",
    "            arc_nll= self._construct_loss(\n",
    "                arc_scores=arc_scores, arc_tags=arc_tags, mask=mask\n",
    "            )\n",
    "            # same here with no arc relations ; keep all anyway to prevent ubgs downstream (TODO: coherent renaming)\n",
    "            output_dict[\"loss\"] = arc_nll \n",
    "            output_dict[\"arc_loss\"] = arc_nll\n",
    "            \n",
    "\n",
    "            # Make the arc tags not have negative values anywhere\n",
    "            # (by default, no edge is indicated with -1).\n",
    "            # NB re chat: probably not useful, but kept as a precaution\n",
    "            arc_indices = (arc_tags != -1).float()\n",
    "            tag_mask = mask.unsqueeze(1) & mask.unsqueeze(2)\n",
    "            one_minus_arc_probs = 1 - arc_probs\n",
    "            # We stack scores here because the f1 measure expects a\n",
    "            # distribution, rather than a single value.\n",
    "            self._unlabelled_f1(\n",
    "                torch.stack([one_minus_arc_probs, arc_probs], -1), arc_indices, tag_mask\n",
    "            )\n",
    "        \n",
    "        \n",
    "        return output_dict\n",
    "    # modified / partially tested\n",
    "    def _construct_loss(\n",
    "        self,\n",
    "        arc_scores: torch.Tensor,\n",
    "        arc_tags: torch.Tensor,\n",
    "        mask: torch.BoolTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the arc loss for an adjacency matrix.\n",
    "\n",
    "        # Parameters\n",
    "\n",
    "        arc_scores : `torch.Tensor`, required.\n",
    "            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate a\n",
    "            binary classification decision for whether an edge is present between two words.\n",
    "        arc_tags : `torch.Tensor`, required.\n",
    "            A tensor of shape (batch_size, sequence_length, sequence_length).\n",
    "            The labels for every arc (0/1).\n",
    "        mask : `torch.BoolTensor`, required.\n",
    "            A mask of shape (batch_size, sequence_length), denoting unpadded\n",
    "            elements in the sequence.\n",
    "\n",
    "        # Returns\n",
    "\n",
    "        arc_nll : `torch.Tensor`, required.\n",
    "            The negative log likelihood from the arc loss.\n",
    "        tag_nll : `torch.Tensor`, required.\n",
    "            The negative log likelihood from the arc tag loss.\n",
    "        \"\"\"\n",
    "        arc_indices = (arc_tags != -1).float()\n",
    "        # Make the arc tags not have negative values anywhere\n",
    "        # (by default, no edge is indicated with -1).\n",
    "        arc_tags = arc_tags * arc_indices\n",
    "        arc_nll = self._arc_loss(arc_scores, arc_indices) * mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        \n",
    "        # We want the mask for the tags to only include the unmasked words\n",
    "        # and we only care about the loss with respect to the gold arcs.\n",
    "        tag_mask = mask.unsqueeze(1) * mask.unsqueeze(2) * arc_indices\n",
    "\n",
    "        #batch_size, sequence_length, _, num_tags = arc_tag_logits.size()\n",
    "        #original_shape = [batch_size, sequence_length, sequence_length]\n",
    "        #reshaped_logits = arc_tag_logits.view(-1, num_tags)\n",
    "        reshaped_tags = arc_tags.view(-1)\n",
    "        #tag_nll = (\n",
    "        #    self._tag_loss(reshaped_logits, reshaped_tags.long()).view(original_shape) * tag_mask\n",
    "        #)\n",
    "\n",
    "        valid_positions = tag_mask.sum()\n",
    "\n",
    "        arc_nll = arc_nll.sum() / valid_positions.float()\n",
    "        #tag_nll = tag_nll.sum() / valid_positions.float()\n",
    "        return arc_nll#, tag_nll\n",
    "    \n",
    "    \n",
    "    # modified / untested\n",
    "    # Warning: the initial Dozat & Manning implementation filter prediction with threshold here instead\n",
    "    # of decoding, but not in latest 1.1 allennlp version\n",
    "    #-------\n",
    "    # this is supposed to be called when ? doc unclear. says with model.forward_on_instances but does not seem\n",
    "    # to be the case (see tests below)\n",
    "    #-------\n",
    "    # no method for Model in used version (0.9?)\n",
    "    #@overrides\n",
    "    def make_output_human_readable(\n",
    "        self, output_dict: Dict[str, torch.Tensor]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        #breakpoint()\n",
    "        arc_probs = output_dict[\"arc_probs\"].cpu().detach().numpy()\n",
    "        mask = output_dict[\"mask\"]\n",
    "        lengths = get_lengths_from_binary_sequence_mask(mask)\n",
    "        arcs = []\n",
    "        arc_tags = []\n",
    "        for instance_arc_probs, length in zip(\n",
    "            arc_probs, lengths\n",
    "        ):\n",
    "\n",
    "            arc_matrix = instance_arc_probs > self.edge_prediction_threshold\n",
    "            edges = []\n",
    "            #edge_tags = []\n",
    "            for i in range(length):\n",
    "                for j in range(length):\n",
    "                    if arc_matrix[i, j] == 1:\n",
    "                        edges.append((i, j))\n",
    "                        #tag = instance_arc_tag_probs[i, j].argmax(-1)\n",
    "                        #edge_tags.append(self.vocab.get_token_from_index(tag, \"labels\"))\n",
    "            arcs.append(edges)\n",
    "            #arc_tags.append(edge_tags)\n",
    "\n",
    "        output_dict[\"arcs\"] = arcs\n",
    "        #output_dict[\"arc_tags\"] = arc_tags\n",
    "        return output_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    # modified/partially tested\n",
    "    @staticmethod\n",
    "    def _greedy_decode(\n",
    "        arc_scores: torch.Tensor, \n",
    "        mask: torch.BoolTensor,\n",
    "        prediction_window: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Decodes the head and head tag predictions by decoding the unlabeled arcs\n",
    "        independently for each word and then again, predicting the head tags of\n",
    "        these greedily chosen arcs independently.\n",
    "\n",
    "        # Parameters\n",
    "\n",
    "        arc_scores : `torch.Tensor`, required.\n",
    "            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n",
    "            a distribution over attachments of a given word to all other words.\n",
    "        ###arc_tag_logits : `torch.Tensor`, required.\n",
    "        ###    A tensor of shape (batch_size, sequence_length, sequence_length, num_tags) used to\n",
    "        ###    generate a distribution over tags for each arc.\n",
    "        mask : `torch.BoolTensor`, required.\n",
    "            A mask of shape (batch_size, sequence_length).\n",
    "\n",
    "        # Returns\n",
    "\n",
    "        arc_probs : `torch.Tensor`\n",
    "            A tensor of shape (batch_size, sequence_length, sequence_length) representing the\n",
    "            probability of an arc being present for this edge.\n",
    "        ####arc_tag_probs : `torch.Tensor`\n",
    "        ####    A tensor of shape (batch_size, sequence_length, sequence_length, sequence_length)\n",
    "        ####    representing the distribution over edge tags for a given edge.\n",
    "        \"\"\"\n",
    "        # Mask the diagonal, because we don't self edges.\n",
    "        # WARNING: might not be the case for chats ? -> should be an option\n",
    "        #inf_diagonal_mask = torch.diag(arc_scores.new(mask.size(1)).fill_(-float('inf')))\n",
    "        \n",
    "        # no edges going backwards\n",
    "        triangle_upper_mask = torch.triu(arc_scores.new(mask.size(1),mask.size(1)).fill_(-float('inf')))\n",
    "        # prevent edges between turns more than prediction_window turns appart\n",
    "        up = 1-torch.triu(arc_scores.new(mask.size(1),mask.size(1)).fill_(1),diagonal=prediction_window)\n",
    "        down = 1-torch.tril(arc_scores.new(mask.size(1),mask.size(1)).fill_(1),diagonal=-prediction_window)\n",
    "        diag_mask = torch.log(((up+down)-1))\n",
    "        # up_mask = torch.triu(arc_scores.new_zeros(mask.size(1),mask.size(1),diagonal=-15)\n",
    "        # down_mask = torch.tril(arc_scores.new_zeros(mask.size(1),mask.size(1),diagonal=15)\n",
    "        # away_mask = (up_mask == down_mask)\n",
    "        #arc_scores = arc_scores + inf_diagonal_mask \n",
    "        arc_scores = arc_scores + diag_mask + triangle_upper_mask\n",
    "        \n",
    "        # shape (batch_size, sequence_length, sequence_length, num_tags)\n",
    "        #arc_tag_logits = arc_tag_logits + inf_diagonal_mask.unsqueeze(0).unsqueeze(-1)\n",
    "        # Mask padded tokens, because we only want to consider actual word -> word edges.\n",
    "        # CHAT: this is the wrong torch version lol this does not work/ confusion int/bools\n",
    "        # minus_mask = ~mask.unsqueeze(2)\n",
    "        # CHAT: this should work with torch>1.4\n",
    "        #minus_mask = (mask<1).unsqueeze(2)\n",
    "        minus_mask = (mask.unsqueeze(1) & mask.unsqueeze(2))<1\n",
    "        \n",
    "        arc_scores.masked_fill_(minus_mask, -float('inf'))\n",
    "        # \n",
    "        \n",
    "        #arc_tag_logits.masked_fill_(minus_mask.unsqueeze(-1), -float('inf'))\n",
    "        # shape (batch_size, sequence_length, sequence_length)\n",
    "        arc_probs = arc_scores.sigmoid()\n",
    "        # best arc option: set all but the best arc for a given target to 0 (== best head) \n",
    "        #force_max(arc_probs)\n",
    "        \n",
    "        # shape (batch_size, sequence_length, sequence_length, num_tags)\n",
    "        #arc_tag_probs = torch.nn.functional.softmax(arc_tag_logits, dim=-1)\n",
    "        return arc_probs#, arc_tag_probs\n",
    "    # modified / untested\n",
    "    \n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        precision, recall, f1_measure = self._unlabelled_f1.get_metric(reset)\n",
    "        metrics[\"precision\"] = precision\n",
    "        metrics[\"recall\"] = recall\n",
    "        metrics[\"f1\"] = f1_measure\n",
    "        return metrics\n",
    "\n",
    "    # since human readable not accessible, post process the outputs of\n",
    "    # model.forward_on_instances\n",
    "    # includes prob threshold here\n",
    "    def extract_threshold(self,output_dict):\n",
    "        instance_arc_probs = output_dict[\"arc_probs\"]\n",
    "        mask = output_dict[\"mask\"]\n",
    "        length = len(mask)\n",
    "        if True:\n",
    "            arc_matrix = instance_arc_probs > self.edge_prediction_threshold\n",
    "            edges = []\n",
    "            for i in range(length):\n",
    "                for j in range(length):\n",
    "                    if arc_matrix[i, j] == 1:\n",
    "                        edges.append((i, j))\n",
    "\n",
    "        output_dict[\"arcs\"] = edges\n",
    "        return output_dict\n",
    "\n",
    "    def extract_best(self,output_dict):\n",
    "        best_heads = output_dict[\"arc_probs\"].argmax(axis=1)\n",
    "        best_values = output_dict[\"arc_probs\"].max(axis=1)\n",
    "        heads = list(zip(range(len(best_heads)),zip(best_heads,best_values)))\n",
    "        output_dict[\"arcs\"] = heads\n",
    "        return output_dict\n",
    "    \n",
    "    def predict_graph(self,instances,data_frame=False,best=False):\n",
    "        \"\"\"apply model on a bunch of instances\n",
    "\n",
    "        data_frame: if True, produce a pandas DataFrame, otherwise just add explicit edges to model output\n",
    "\n",
    "        best: if True, take only best head for each turn \n",
    "        \"\"\"\n",
    "        outputs = self.forward_on_instances(instances)\n",
    "        \n",
    "        if best:    \n",
    "            outputs = [model.extract_best(x) for x in outputs]\n",
    "        else:\n",
    "            outputs = [model.extract_threshold(x) for x in outputs]\n",
    "            \n",
    "        if not data_frame: \n",
    "            return outputs\n",
    "\n",
    "        # save only positive cases for now\n",
    "        frame = []\n",
    "        for one in outputs:\n",
    "            source = one[\"metadata\"][\"file_source\"]\n",
    "            start_idx = one[\"metadata\"][\"first_line\"]\n",
    "            #print(source,start_idx)\n",
    "            for (i,j) in one[\"arcs\"]:\n",
    "                if best: \n",
    "                    tid, label = j\n",
    "                else:\n",
    "                    tid, label = j,1\n",
    "                frame.append((source,i+start_idx,tid+start_idx,\"<sentence>\",\"<sentence>\",label))\n",
    "        return pds.DataFrame(frame,columns=[\"source_file\",\"source\",\"target\",\"sentence1\",\"sentence2\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_representation_dim = 50 \n",
    "feature_size = 4\n",
    "\n",
    "model = ChatGraphParser(vocab,word_embeddings,\n",
    "                        turn_encoder,chat_encoder,arc_representation_dim,\n",
    "                        use_features = True,\n",
    "                        turn_feature_size = 1,\n",
    "                        feature_size = feature_size,\n",
    "                        pair_feature_size = 1+feature_size,\n",
    "                        prediction_window = 10,\n",
    "                        positive_class_weight = 1000,\n",
    "                        debug=False,\n",
    "                        edge_prediction_threshold=0.55)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "\n",
    "#cuda_device = -1\n",
    "    \n",
    "    \n",
    "# not used yet, but ready\n",
    "from allennlp.training.optimizers import Optimizer \n",
    "trainer_cfg = Params({\n",
    "        \"cuda_device\": cuda_device,\n",
    "        \"grad_norm\": 5,\n",
    "        \"num_epochs\": 100,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"dense_sparse_adam\",\n",
    "            \"betas\": [\n",
    "                0.9,\n",
    "                0.9\n",
    "            ]\n",
    "        },\n",
    "        \"patience\": 50,\n",
    "})\n",
    "opt_cfg = trainer_cfg.pop(\"optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# not tested\n",
    "#optimizer = Optimizer.from_params(model_parameters=model.parameters(),\n",
    "#                                  params=opt_cfg)\n",
    "\n",
    "# \"lines\" <-> \"tokens\"\n",
    "iterator = BucketIterator(batch_size=8,\n",
    "                          sorting_keys=[(\"lines\",\"list_num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "\n",
    "# requires Allennlp > 1.0\n",
    "#from allennlp.training.trainer import EpochCallback\n",
    "#class Epoch_nb(EpochCallback):\n",
    "#    \n",
    "#    def __call__(\n",
    "#          self,\n",
    "#          trainer: \"GradientDescentTrainer\",\n",
    "#          metrics: Dict[str, Any],\n",
    "#          epoch: int\n",
    "#      ) -> None:\n",
    "#        print(\"Epoch nb %d\"%epoch,end=\"\\t\")\n",
    "\n",
    "    \n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  num_epochs=20,\n",
    "                  grad_norm=5,\n",
    "                  patience=10,\n",
    "                  cuda_device = cuda_device,\n",
    "                  train_dataset=train_instances,\n",
    "                  validation_dataset=dev_instances,\n",
    "                  should_log_parameter_statistics = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "precision: 0.0717, recall: 0.6243, f1: 0.1286, loss: 946.3008 ||: 100%|██████████| 20/20 [00:14<00:00,  1.55it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 659.8169 ||: 100%|██████████| 2/2 [00:00<00:00,  7.21it/s]\n",
      "precision: 0.0718, recall: 0.6258, f1: 0.1287, loss: 923.8093 ||: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 644.6534 ||: 100%|██████████| 2/2 [00:00<00:00,  6.86it/s]\n",
      "precision: 0.0718, recall: 0.6263, f1: 0.1288, loss: 922.8437 ||: 100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 650.4512 ||: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\n",
      "precision: 0.0718, recall: 0.6251, f1: 0.1287, loss: 930.9834 ||: 100%|██████████| 20/20 [00:14<00:00,  1.56it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 637.6427 ||: 100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      "precision: 0.0718, recall: 0.6263, f1: 0.1289, loss: 913.4115 ||: 100%|██████████| 20/20 [00:14<00:00,  1.41it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 642.6013 ||: 100%|██████████| 2/2 [00:00<00:00,  6.90it/s]\n",
      "precision: 0.0717, recall: 0.6249, f1: 0.1287, loss: 914.1701 ||: 100%|██████████| 20/20 [00:14<00:00,  1.13it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 631.4719 ||: 100%|██████████| 2/2 [00:00<00:00,  6.89it/s]\n",
      "precision: 0.0718, recall: 0.6246, f1: 0.1288, loss: 906.5536 ||: 100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n",
      "precision: 0.0834, recall: 0.7151, f1: 0.1494, loss: 636.5725 ||: 100%|██████████| 2/2 [00:00<00:00,  6.97it/s]\n",
      "precision: 0.0714, recall: 0.6226, f1: 0.1281, loss: 895.8495 ||:  95%|█████████▌| 19/20 [00:13<00:00,  1.71it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-7bd9b83fd35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%pdb off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# get peak of memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mbatch_loss\u001b[0;34m(self, batch_group, for_training)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-65ec3013ff3a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lines, arcs, rel_features, offsets, is_server, metadata)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# distribution, rather than a single value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             self._unlabelled_f1(\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mone_minus_arc_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_probs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marc_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/training/metrics/fbeta_measure.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, predictions, gold_labels, mask)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mgold_labels_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgold_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgold_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_labels_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%pdb off\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of irc_chat_reader_modif failed: Traceback (most recent call last):\n",
      "  File \"/home/muller/miniconda3/envs/allennlp2/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/muller/miniconda3/envs/allennlp2/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 376, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/muller/miniconda3/envs/allennlp2/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/muller/miniconda3/envs/allennlp2/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/muller/Devel/Linto/linto-sp5-conversational-thread-extraction/experiments/irc_chat_reader_modif.py\", line 34, in <module>\n",
      "    class ChatReader(DatasetReader):\n",
      "  File \"/home/muller/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/common/registrable.py\", line 66, in add_subclass_to_registry\n",
      "    raise ConfigurationError(message)\n",
      "allennlp.common.checks.ConfigurationError: 'Cannot register chat_reader as DatasetReader; name already in use for ChatReader'\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab': Vocabulary with namespaces:  tokens, Size: 45682 || Non Padded Namespaces: {'*tags', '*labels'},\n",
       " '_batch_size': 32,\n",
       " '_max_instances_in_memory': None,\n",
       " '_instances_per_epoch': None,\n",
       " '_maximum_samples_per_batch': None,\n",
       " '_cache_instances': False,\n",
       " '_cache': defaultdict(list, {}),\n",
       " '_track_epoch': False,\n",
       " '_epochs': defaultdict(int, {140455247675056: 11, 140454605160928: 11}),\n",
       " '_cursors': {},\n",
       " '_sorting_keys': [('lines', 'list_num_tokens')],\n",
       " '_padding_noise': 0.1,\n",
       " '_biggest_batch_first': False,\n",
       " '_skip_smaller_batches': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.iterator.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2497it [00:00, 2609.87it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_instances = reader.read(\"../data/dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.92 GiB (GPU 0; 10.73 GiB total capacity; 3.17 GiB already allocated; 1.08 GiB free; 3.19 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f96ec16b4e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arc_probs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arc_probs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minstance_separated_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-65ec3013ff3a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lines, arcs, rel_features, offsets, is_server, metadata)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# chat turns fetching embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# turns_embedding tensor is (batch_size x turns x max tokens x token embedding size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mturns_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_wrapping_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# encoding turns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/allennlp2/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mtoken_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0membedded_representations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# This is some unusual logic, it needs a custom from_params.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.92 GiB (GPU 0; 10.73 GiB total capacity; 3.17 GiB already allocated; 1.08 GiB free; 3.19 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "outputs = model.forward_on_instances(dev_instances)\n",
    "output_dict = outputs[0]\n",
    "best_heads = output_dict[\"arc_probs\"].argmax(axis=1)\n",
    "best_values = output_dict[\"arc_probs\"].max(axis=1)\n",
    "heads = list(zip(range(len(best_heads)),zip(best_heads,best_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict_graph(dev_instances,best=True,data_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probing the gold annotations to see how many multiple attachements\n",
    "gold = {}\n",
    "for (i,j) in dev_instances[1][\"arcs\"].indices:\n",
    "    if i in gold:\n",
    "        gold[i].append(j)\n",
    "    else:\n",
    "        gold[i] = [j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([len(gold[i]) for i in gold])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "nb = 200\n",
    "turn_key = \"lines\"\n",
    "for i,turn in enumerate(dev_instances[0][turn_key][start:start+nb]):\n",
    "    k = i + start\n",
    "    head = gold.get(k,\"NONE\")\n",
    "    if head == [k]: head = \"SELF-LOOP\"\n",
    "    print(k,\"->\",head,turn.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dev_instances[0][\"arcs\"].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (from tutorial) Here's how to save the model.\n",
    "with open(\"/tmp/model.th\", 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "vocab.save_to_files(\"/tmp/vocabulary\")\n",
    "# And here's how to reload the model.\n",
    "vocab2 = Vocabulary.from_files(\"/tmp/vocabulary\")\n",
    "model2 = ChatGraphParser(vocab2,word_embeddings,\n",
    "                        turn_encoder,chat_encoder,arc_representation_dim,\n",
    "                        prediction_window = 10,\n",
    "                        positive_class_weight = 1000,\n",
    "                        debug=False,\n",
    "                        edge_prediction_threshold=0.6)\n",
    "with open(\"/tmp/model.th\", 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))\n",
    "if cuda_device > -1:\n",
    "    model2.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
