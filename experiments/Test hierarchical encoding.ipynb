{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from allennlp.data.dataset_readers import SnliReader\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "# to be checked: reads a text as a list of sentences\n",
    "from allennlp.data.dataset_readers import TextClassificationJsonReader\n",
    "from allennlp.data.fields import Field\n",
    "from allennlp.data.fields import LabelField\n",
    "from allennlp.data.fields import TextField, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "from allennlp.modules import TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "\n",
    "\n",
    "from allennlp.modules import Seq2VecEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.common import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-977386625581>\", line 1, in <module>\n",
      "    from dummy_chat_reader import ChatReader\n",
      "  File \"/home/sdelecra/work/linto-sp5-conversational-thread-extraction/experiments/dummy_chat_reader.py\", line 25, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "ModuleNotFoundError: No module named 'sentence_transformers'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/sdelecra/anaconda3/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from dummy_chat_reader import ChatReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SingleIdTokenIndexer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c199c08f1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_indexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSingleIdTokenIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"word_splitter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SingleIdTokenIndexer' is not defined"
     ]
    }
   ],
   "source": [
    "token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "tokenizer_cfg = Params({\"word_splitter\": {\"language\": \"en\"}})\n",
    "\n",
    "tokenizer = Tokenizer.from_params(tokenizer_cfg)\n",
    "\n",
    "\n",
    "reader = ChatReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers=token_indexers,\n",
    "    )\n",
    "train_instances = reader.read(\"./train_dummy.tsv\")\n",
    "vocab = Vocabulary.from_instances(train_instances)\n",
    "\n",
    "\n",
    "for i in train_instances:\n",
    "    #print(i)\n",
    "    i[\"lines\"].index(vocab)\n",
    "    print(i[\"lines\"].get_padding_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [another, chat, starts],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f940d963450>},\n",
       " '_indexed_tokens': {'tokens': [7, 6, 12]},\n",
       " '_indexer_name_to_indexed_token': {'tokens': ['tokens']},\n",
       " '_token_index_to_indexer_name': {'tokens': 'tokens'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[\"lines\"][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "400000it [00:02, 160810.00it/s]\n"
     ]
    }
   ],
   "source": [
    "turn_encoder_cfg = Params({\"type\":\"gru\",'input_size': 100, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "#can be changed dynamically encoder_cfg[\"type\"] = \"lstm\"\n",
    "# warning: if bidirectional, state output dimension is hidden_size x 2 -> model doesn't know that\n",
    "\n",
    "turn_encoder = Seq2VecEncoder.from_params(turn_encoder_cfg)\n",
    "turn_encoder.hidden_size = turn_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "chat_encoder_cfg = Params({\"type\":\"gru\",'input_size': 50, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "chat_encoder = Seq2VecEncoder.from_params(chat_encoder_cfg)\n",
    "chat_encoder.hidden_size = chat_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "\n",
    "glove_text_field_embedder = Embedding.from_params(vocab,Params({\"pretrained_file\": \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                                                          \"embedding_dim\": 100,\n",
    "                                                          \"trainable\": False\n",
    "}))\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=100)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "\n",
    "# not used for now\n",
    "trainer_cfg = Params({\"iterator\": {\"type\": \"bucket\",\n",
    "                                   \"batch_size\": 32\n",
    "},\n",
    "                      \"trainer\": {\n",
    "                          \"optimizer\": {\n",
    "                              \"type\": \"adam\"\n",
    "                          },\n",
    "                          \"num_epochs\": 3,\n",
    "                          \"patience\": 10,\n",
    "                          \"cuda_device\": -1\n",
    "                      }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_encoder import HierarchicalChatClassification\n",
    "\n",
    "model = HierarchicalChatClassification(vocab,word_embeddings,turn_encoder,chat_encoder)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2,sorting_keys=[(\"lines\",\"list_num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_instances,\n",
    "                  should_log_parameter_statistics = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000, loss: 0.6784 ||: 100%|██████████| 1/1 [00:00<00:00, 99.30it/s]\n",
      "accuracy: 1.0000, loss: 0.5810 ||: 100%|██████████| 1/1 [00:00<00:00, 88.41it/s]\n",
      "accuracy: 1.0000, loss: 0.5029 ||: 100%|██████████| 1/1 [00:00<00:00, 112.61it/s]\n",
      "accuracy: 1.0000, loss: 0.4389 ||: 100%|██████████| 1/1 [00:00<00:00, 120.47it/s]\n",
      "accuracy: 1.0000, loss: 0.3854 ||: 100%|██████████| 1/1 [00:00<00:00, 118.41it/s]\n",
      "accuracy: 1.0000, loss: 0.3403 ||: 100%|██████████| 1/1 [00:00<00:00, 93.00it/s]\n",
      "accuracy: 1.0000, loss: 0.3018 ||: 100%|██████████| 1/1 [00:00<00:00, 108.93it/s]\n",
      "accuracy: 1.0000, loss: 0.2688 ||: 100%|██████████| 1/1 [00:00<00:00, 130.66it/s]\n",
      "accuracy: 1.0000, loss: 0.2405 ||: 100%|██████████| 1/1 [00:00<00:00, 111.60it/s]\n",
      "accuracy: 1.0000, loss: 0.2159 ||: 100%|██████████| 1/1 [00:00<00:00, 135.64it/s]\n",
      "accuracy: 1.0000, loss: 0.1946 ||: 100%|██████████| 1/1 [00:00<00:00, 96.25it/s]\n",
      "accuracy: 1.0000, loss: 0.1761 ||: 100%|██████████| 1/1 [00:00<00:00, 149.83it/s]\n",
      "accuracy: 1.0000, loss: 0.1599 ||: 100%|██████████| 1/1 [00:00<00:00, 143.95it/s]\n",
      "accuracy: 1.0000, loss: 0.1457 ||: 100%|██████████| 1/1 [00:00<00:00, 109.82it/s]\n",
      "accuracy: 1.0000, loss: 0.1333 ||: 100%|██████████| 1/1 [00:00<00:00, 111.17it/s]\n",
      "accuracy: 1.0000, loss: 0.1223 ||: 100%|██████████| 1/1 [00:00<00:00, 111.98it/s]\n",
      "accuracy: 1.0000, loss: 0.1125 ||: 100%|██████████| 1/1 [00:00<00:00, 113.88it/s]\n",
      "accuracy: 1.0000, loss: 0.1039 ||: 100%|██████████| 1/1 [00:00<00:00, 115.78it/s]\n",
      "accuracy: 1.0000, loss: 0.0962 ||: 100%|██████████| 1/1 [00:00<00:00, 106.79it/s]\n",
      "accuracy: 1.0000, loss: 0.0893 ||: 100%|██████████| 1/1 [00:00<00:00, 129.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_cpu_memory_MB': 331.64,\n",
       " 'peak_gpu_0_memory_MB': 959,\n",
       " 'training_duration': '0:00:00.581604',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 1.0,\n",
       " 'training_loss': 0.08934460580348969,\n",
       " 'training_cpu_memory_MB': 331.64,\n",
       " 'training_gpu_0_memory_MB': 959}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
